\subsection{Solver Evaluation}
ALIAS solver will be evaluated in terms of the functional and non-functional requirements discussed in section \ref{label:projectRequirements}. 

\subsubsection{Functionality Evaluation}
Although original implementation of ALIAS provided a lot of functionality, the core of the application has been reimplemented to explore different approaches of computing abstract argumentation semantics. 

In terms of functionality, user can easily use ALIAS to create new argumentation framework, add arguments and attacks and perform tasks for three main extensions: complete, preferred and stable. The implementation has covered all critical functionalities identified and categorized as \textit{Must Haves} in section \ref{label:projectRequirements}. Furthermore, additional functionality in terms of available tasks per each extensions has been implemented. User can perform following tasks:
\begin{itemize}
	\item Compute all extensions
	\item Compute some extensions
	\item Given argument calculate if the argument is credulously accepted 
	\item Given argument calculate if the argument is skeptically accepted
\end{itemize}
Only first task has been classified as critical for all three main extensions. The remaining tasks are categorized as \textit{Should have}. 

Table \ref{table:tasksSupportedBySolvers} shows the comparison of ALIAS functionality to existing solvers submitted to ICCMA 2017 competition. Only half of the submitted solvers offer full functionality in terms of the possible extensions. Other half implements only selection of extensions, for example argmat-clpb can only compute all tasks from complete, stable and grounded extension, while ASPrMin solver only all preferred extensions. Furthermore, it can be seen that from the solvers implementing only some of the extensions, complete, preferred and stable extensions are the most common one.
	
\begin{sidewaystable}
	\caption{Tasks supported by solvers}
	\label{table:tasksSupportedBySolvers}
	\resizebox{\paperwidth}{!}{
		\begin{tabular}{|p{.2\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|p{.032\textwidth}|}
			\hline
			& D3 & \multicolumn{4}{l|}{CO} & \multicolumn{4}{l|}{PR} & \multicolumn{4}{l|}{ST} & \multicolumn{4}{l|}{SST} & \multicolumn{4}{l|}{STG} & \multicolumn{2}{l|}{GR} & \multicolumn{2}{l|}{ID} \\ \hline
			& & DC   & DS   & SE  & EE  & DC   & DS   & SE  & EE  & DC   & DS   & SE  & EE  & DC   & DS   & SE   & EE  & DC   & DS   & SE   & EE  & DC   & SE   & DC   & SE   \\ \hline
			\rowcolor{grey}
			ALIAS   & & 1 & 1 & 1   & 1   & 1  & 1  & 1 &1  & 1 & 1 & 1   & 1   &   &   &   &  &   &   &   &  &     &     &   &   \\ \hline
			argmat-clpb   & & 1 & 1 & 1   & 1   &   &   &  &  & 1 & 1 & 1   & 1   &   &   &   &  &   &   &   &  & 1    & 1    &   &   \\ \hline
			argmat-dvisat & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   &   &   &   &  &   &   &   &  & 1    & 1    & 1    & 1    \\ \hline
			argmat-mpg & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			argmat-sat & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			ArgSemSAT  & & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   &   &   &   &  & 1    & 1    &   &   \\ \hline
			ArgTools   & & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			ASPrMin    & &   &   &  &  &   &   &  & 1   &   &   &  &  &   &   &   &  &   &   &   &  &   &   &   &   \\ \hline
			cegartix   & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			Chim√¶rarg & &   &   &  &  &   &   &  & 1   &   &   &  & 1   &   &   &   &  &   &   &   &  &   &   &   &   \\ \hline
			ConArg  & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			CoQuiAAS   & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			EqArgSolver   & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   &   &   &   &  &   &   &   &  & 1    & 1    &   &   \\ \hline
			gg-sts  & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			goDIAMOND  & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
			heureka    & & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   &   &   &   &  &   &   &   &  & 1    & 1    &   &   \\ \hline
			pyglaf  & 1  & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1   & 1   & 1 & 1 & 1 & 1   & 1 & 1 & 1 & 1   & 1    & 1    & 1    & 1    \\ \hline
		\end{tabular}
	}
\end{sidewaystable}

\subsubsection{Performance Evaluation}
Performance evaluation will consist of evaluation of each approach in comparison to other approaches implemented and in comparison to the results of certain solvers from ICCMA 2017 competition. In order for comparison to be valid, Pyglaf \citep{pyglaf}, Argsem-sat \citep{argsemsat} and gg-sts \citep{gg-sts} solvers have been tested on the same machine as ALIAS with the same argumentation frameworks from appendix \ref{appendix:benchmarkFiles}.


\paragraph{Approaches} 
Two different approaches for computing abstract argumentation semantics have been tried throughout this project: direct and SAT based approach. Furthermore, the direct approach of computing Maximal Conflict Free Sets have been implemented using three different data structures. This has been done to try to improve the proposed solution. 

Based on the results from section \ref{section:stableExtensionResults} and \ref{section:preferredExtensionResults}, it can be seen that the least successful approach was a direct approach using PyTables. Solver only managed to compute extension for the smallest and easiest argumentation frameworks within the specified 20 minutes time limit. However, the idea of PyTables is to use the hard drive for a data store. All benchmark testing has been performed using hard disk drive, thus it will reduce the performance. The solver could have performed better if latest generation of solid state drive would have been used. 

Although the sets and dictionaries approaches for Maximal Conflict Free Sets creation produced results for large number of provided argumentation frameworks, they have a massive overhead - memory usage. The tests were performed on the machine with 8GB RAM available for the solver and another 8GB swap memory. Both of the solutions: sets and dictionaries, were running out of memory space for any framework with 20 or more arguments. Since this approach is combinatorial and  generates all the possible maximal conflict free sets first, the number of combinations is extremely high. Hence, storing all possible sets is memory expensive as in the worst case scenario it will generate $2^n$ records, where $n$ is the number of arguments in the framework.

SAT based approach tends to outperform other solutions for frameworks it managed to compute. As can be seen in figure \ref{fig:stableFinalResults}, half of the timings for SAT solver approach are considerably faster than other approaches when computing stable extensions. Only in 4 instances, this approach was slower than approach using dictionaries. On the other hand, figure \ref{fig:preferredFinalResults} shows that SAT based approach was considerably slower for preferred extensions. 

However, as shown in the section \ref{section:satSolver}, SAT based approach uses a simple encodings for the admissible sets. While, for direct approach of computing Maximal Conflict Free sets, the only changes in the implementation can be done to fine tune and optimize the solution, the SAT based approach has bigger possibilities. Implementing more complex CNF encodings to reduce the search space will greatly improve the performance of ALIAS.

\paragraph{Other Solvers Comparison}



\subsubsection{Ease of Use}
Although the SAT solver used by ALIAS is implemented in C++, ALIAS itself has been implemented using pure Python programming language. Responsibility for compiling and building PicoSAT solver has been moved to the Python wrapper package - PycoSAT. Hence, once the setup script is run and all dependencies installed automatically, user can start working with ALIAS by simply importing the module to Python environment.

