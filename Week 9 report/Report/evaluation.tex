Evaluation is the critical part of the project. Hence, the developed software will need to be evaluated based on three properties: performance, scalability and correctness. 

\section{Performance Evaluation}
In order to evaluate the performance of the proposed solution benchmark tests will be performed. The performance of the software will be compared to solvers submitted for the second edition of International Competition on Computational Models of Argumentation (http://argumentationcompetition.org/). This will allow to appropriately evaluate the performance in comparison to existing solutions. In order to perform benchmark testing the base line of execution times will be created based on the solvers submitted to ICCMA 2017. This tests will include computing semantics from provided argumentation frameworks and evaluating from the given argument and argumentation framework, if the argument is credulously or sceptically inferred in any of the semantics. Seven main tracks are identified, where each track will represent different semantic. Following semantics will be used for benchmark testing:
\begin{enumerate}
	\item{Complete Semantics - CO}
	\item{Preferred Semantics - PR}
	\item{Stable Semantics - ST}
	\item{Semi-Stable Semantics - SST}
	\item{Stage Semantics - STG}
	\item{Grounded Semantics - GR}
	\item{Ideal Semantics - ID}
\end{enumerate}

For each track the system will need to solve following reasoning problems, with the exception for Grounded and Ideal semantics where only A and C are applicable:
\begin{enumerate}
	\item{Given an abstract argumentation framework, determine some extension\\SE-$\sigma$: Given F=(A,R) return some set E $\subseteq$ A that is a $\sigma$-extension}
	\item{Given an abstract argumentation framweork, determine all extensions\\EE-$\sigma$: Given F=(A,R) enumerate all sets E $\subseteq$ A that are $\sigma$-extensions}
	\item{Given an abstract argumentation framework and some argument, decide whether the given argument is credulously inferred\\DC-$\sigma$ : Given F=(A,R), a $\in$ A decide whether a is credulously accepted under $\sigma$}
	\item{Given an abstract argumentation framework and some argument, decide whether the given argument is skeptically inferred\\DS-$\sigma$: Given F=(A,R), a $\in$ A decide whether a is skeptically accepted under $\sigma$}
\end{enumerate}

There are 4 different sets of argumentation frameworks that will be used, where all of them were used for testing solvers submitted to ICCMA 2017. Each set consist a number of abstract argumentation frameworks in formats \textit{apx} and \textit{tgf}. Furthermore, each group of argumentation frameworks are used for a specific tasks. Graph representation mapped in yEd of one of the argumentation frameworks can be seen in appendix \ref{appendix:aficcma}. The list of relations between the sets and the tasks is as follow:
\begin{itemize}
	\item{Set A - DS-PR, EE-PR, EE-CO, DS-SST, DC-SST, SE-SST, EE-SST, DS-STG, DC-STG, SE-STG, EE-STG}
	\item{Set B - DS-ST, DC-ST, SE-ST, EE-ST, DC-PR, SE-PR, DC-CO}
	\item{Set C - DS-CO, SE-CO, DC-GR, SE-GR}
	\item{Set D - DC-ID, SE-ID}
\end{itemize}

As mentioned above, prior to running the benchmark tests, solvers submitted to ICCMA 2017 will need to be tested with the available argumentation frameworks to evaluate their performance in the given environment. Full list of those solvers can be found in table \ref{table:ICCMA2017Submissions} with the tasks supported by them in table \ref{table:tasksSupportedBySolvers} in appendix \ref{appendix:ICCMASubmissions}.

\section{Scalability Evaluation}
Scalability is another important property that the software will be evaluated on. In order to properly test it, the software will be required to perform the same tasks as described above, but for the sets of larger argumentation frameworks. The argumentation frameworks will be prepared in advanced using one of the generators for random argumenation frameworks, such as AFBenchGen2 \citep{ceruttigenerator}. This will allow to specify the number of arguments required per framework. \\
For the purpose of this test, set of argumentation frameworks will be produced with increasing number of arguments. This will allow to test the solution in terms of scalability of computing the semantics in larg data sets. Comparing the time taken to perform the tasks with argumentation frameworks of different sizes will allow to evaluate how the software scales.

\section{Correctness Evaluation}
Correctness of the software relates to the correctness of the output of computation. In order to evaluate it the set tasks described above will be executed, and the output of the software verified. However, since the generated argumentation frameworks only consists of arguments and attacks, the correct output will need to be verified manually. Hence, the size of the argumentation frameworks need to be of a reasonable size to evaluate them manually. \\
Testing correctness of the software proves to be problematic with larger data sets. Hence, further research will need to be performed to optimize the this tasks. Type of evaluation of correctness of existing solvers will be explored. Furthermore, formal verification methods for software might be used. Since they comprise a set of techniques for proving the correctness of a software for a possible combination of input values \citep{murthy2009software},it will help in the evaluation. Different approaches will be explored prior to testing.